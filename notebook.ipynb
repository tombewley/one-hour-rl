{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25334ff2",
   "metadata": {},
   "source": [
    "# An Introduction to Reinforcement Learning\n",
    "\n",
    "## [Tom Bewley](https://tombewley.com/) & [Scott Jeen](https://enjeeneer.io/)\n",
    "\n",
    "### 24/02/2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae45577",
   "metadata": {},
   "source": [
    "# 1 | Markov Decision Processes: A Model of Sequential Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858da42",
   "metadata": {},
   "source": [
    "## 1.1. MDP (semi-)Formalism "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b8dc4",
   "metadata": {},
   "source": [
    "In reinforcement learning (RL), an *agent* takes *actions* in an *environment* to change its state, with the goal of maximising the expected sum of future *rewards*. We formalise this interaction as an agent-environment loop, mathematically described as a Markov Decision Process (MDP). \n",
    "\n",
    "<img src='https://github.com/enjeeneer/sutton_and_barto/blob/main/images/chapter3_1.png?raw=true' width='700'>\n",
    "\n",
    "MDPs break the I.I.D. data assumption of supervised and unsupervised learning; the agent *causally influences* the data it sees through its choice of actions. However, one assumption we do make is the *Markov property*, which says that the state representation captures *all relevent information* from the past. Formally, state transitions depend only on the most recent state and action,\n",
    "$$\n",
    "\\mathbb{P}[S_{t+1} | S_t,A_t] = \\mathbb{P}[S_{t+1} | S_1,A_1 \\ldots\\, S_t,A_t],\n",
    "$$\n",
    "and rewards depend only on the most recent transition,\n",
    "$$\n",
    "\\mathbb{P}[R_{t+1} | S_t,A_t,S_{t+1}] = \\mathbb{P}[R_{t+1} | S_1,A_1 \\ldots\\, S_t,A_t,S_{t+1}].\n",
    "$$\n",
    "- Note: different sources use different notation here, but this is the most general.\n",
    "\n",
    "In some MDPs, a subset of states are designated as *terminal* (or *absorbing*). The agent-environment interaction loop ceases once a terminal state is reached.\n",
    "\n",
    "The goal of an RL agent is to pick actions that maximise the discounted cumulative sum of future rewards, also known as the *return* $G_t$:\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots + \\gamma^{T-t-1}R_{T},\n",
    "$$\n",
    "where $\\gamma\\in[0,1]$ is a discount factor and $T$ is the time of termination (may be $\\infty$).\n",
    "\n",
    "To do so, it needs the ability to forecast the reward-getting effect of taking each action $A$ in each state $S$, potentially many timesteps into the future. This *temporal credit assignment* problem is one of the key factors that makes RL so challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04bbeb6",
   "metadata": {},
   "source": [
    "## 1.2 MDP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9e984",
   "metadata": {},
   "source": [
    "Here's a simple MDP (courtesy of David Silver @ DeepMind/UCL), which we'll be using throughout this course.\n",
    "- White circle: non-terminal state\n",
    "- White square: terminal state\n",
    "- Black circle: action\n",
    "- <span style=\"color:green\">Green:</span> reward (depends only on $S_{t+1}$ here)\n",
    "- <span style=\"color:blue\">Blue:</span> state transition probability\n",
    "- <span style=\"color:red\">Red:</span> action probability for an exemplar policy\n",
    "- Note: edges with probability $1$ are unlabelled\n",
    "\n",
    "<img src='https://github.com/tombewley/one-hour-rl/blob/main/images/student-mdp.svg?raw=true' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c686859",
   "metadata": {},
   "source": [
    "## 1.3 Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef693937",
   "metadata": {},
   "source": [
    "[Open AI Gym](https://gym.openai.com/) provides a unified framework for testing and comparing RL algorithms, and offers a suite of MDPs that researchers can use to benchmark their work. It's important to be familiar with the conventions of Gym, because almost all modern RL code is built to work with it. Gym environment classes have two key methods:\n",
    "\n",
    "- `mdp.reset()`: resets the MDP to an initial state $S_0$ according to an initialisation distribution.\n",
    "- `mdp.step(action)` : takes an action $A_t$, combines with the current environment state $S_t$, produces the next state $S_{t+1}$ and delivers the agent a scalar reward $R_{t+1}$.\n",
    "\n",
    "A Gym-compatible class for the student MDP shown above can be found in `mdp.py` in this repository. Let's import it now and explore what it can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import StudentMDP\n",
    "mdp = StudentMDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788fae80",
   "metadata": {},
   "source": [
    "Firstly, we'll have a look at the initialisation probabilities and the behaviour of `mdp.reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mdp.initial_probs())\n",
    "mdp.reset()\n",
    "print(mdp.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f92f8b",
   "metadata": {},
   "source": [
    "Next, let's check which actions are available in this initial state, and the action-dependent transition probabilities.\n",
    "- Reminder: the Markov property dictates that transition probabilities depend *only* on the current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mdp.action_space(mdp.state))\n",
    "print(mdp.transition_probs(mdp.state, \"Study\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f400556",
   "metadata": {},
   "source": [
    "Calling `mdp.step(action)` samples and returns the next state $S_{t+1}$, alongside a scalar reward $R_{t+1}$.\n",
    "\n",
    "Let's try calling this method repeatedly. What's happening here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1611b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, _, _ = mdp.step(\"Study\") \n",
    "print(state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da95376",
   "metadata": {},
   "source": [
    "Transitions out of the `Pub` state are *stochastic*; they go to one of the three classes with specified probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea7350",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.state = \"Pub\"\n",
    "print(mdp.action_space(mdp.state))\n",
    "print(mdp.transition_probs(mdp.state, \"Have a pint\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74297464",
   "metadata": {},
   "source": [
    "In this state, the behaviour of `mdp.step(action)` is non-deterministic, even for a constant action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.state = \"Pub\" # Note that we're resetting the state to Pub each time\n",
    "state, reward, _, _ = mdp.step(\"Have a pint\")\n",
    "print(state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f67c8",
   "metadata": {},
   "source": [
    "This MDP has just one terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f94c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mdp.terminal_states())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069bebe",
   "metadata": {},
   "source": [
    "`mdp.step(action)` also returns a binary `done` flag, which is set to `True` if $S_{t+1}$ is a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.state = \"Class 2\" \n",
    "state, reward, done, _ = mdp.step(\"Fall asleep\")\n",
    "print(state, reward, done)\n",
    "\n",
    "mdp.state = \"Pass\" \n",
    "state, reward, done, _ = mdp.step(\"Fall asleep\")\n",
    "print(state, reward, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346eb1d",
   "metadata": {},
   "source": [
    "Now let's bring an agent into the mix, and give it the exemplar policy shown in the diagram above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "agent = Agent(mdp) \n",
    "agent.policy = {\n",
    "    \"Class 1\":  {\"Study\": 0.5, \"Go on Facebook\": 0.5},\n",
    "    \"Class 2\":  {\"Study\": 0.8, \"Fall asleep\": 0.2},\n",
    "    \"Class 3\":  {\"Study\": 0.6, \"Go to the pub\": 0.4},\n",
    "    \"Facebook\": {\"Keep scrolling\": 0.9, \"Close Facebook\": 0.1},\n",
    "    \"Pub\":      {\"Have a pint\": 1.},\n",
    "    \"Pass\":     {\"Fall asleep\": 1.},\n",
    "    \"Asleep\":   {\"Stay asleep\": 1.}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f70e3",
   "metadata": {},
   "source": [
    "We can query the policy in a similar way to the MDP's properties, and observe its stochastic behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ce8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.policy[\"Class 1\"])\n",
    "print([agent.act(\"Class 1\") for _ in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21146564",
   "metadata": {},
   "source": [
    "Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01628da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.verbose = True\n",
    "state = mdp.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    state, reward, done, info = mdp.step(agent.act(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da9ed3",
   "metadata": {},
   "source": [
    "How \"good\" is this policy? To answer this, we need to calculate its expected return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f27f7",
   "metadata": {},
   "source": [
    "# 2 | Policy Evaluation: The Temporal Difference Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aebd142",
   "metadata": {},
   "source": [
    "==NOTE: INCOMPLETE==\n",
    "\n",
    "For a policy $\\pi$, the *Q value* $Q_\\pi(S_t,A_t)$ is the expected return from taking action $A_t$ in state $S_t$, and following $\\pi$ thereafter. It thus quantifies how well the policy can be expected to perform, starting from this state-action pair. Q values exhibit an elegant recursive relationship known as the *Bellman equation*:\n",
    "$$\n",
    "Q_\\pi(S_t,A_t)=\\sum_{S_{t+1}}\\mathbb{P}[S_{t+1}|S_t,A_t]\\left(\\mathbb{E}[R_{t+1} | S_t,A_t,S_{t+1}]+\\gamma\\times\\sum_{A_{t+1}}\\pi(A_{t+1}|S_{t+1})\\times Q_\\pi(S_{t+1},A_{t+1})\\right).\n",
    "$$\n",
    "\n",
    "i.e. **The Q value for a state-action pair is equal to the immediate reward, plus the $\\gamma$-discounted Q value for the *next* state-action pair, with expectations taken over both the transition function $\\mathbb{P}$ and the policy $\\pi$.**\n",
    "\n",
    "This is a bit of a mouthful, but the Bellman equation is perhaps the single most important thing to understand if you really want to \"get\" reinforcement learning. \n",
    "\n",
    "To gain some intuition for this relationship, here are estimated Q values for the exemplar policy in the student MDP. Here we're using a discount factor of $\\gamma=0.95$\n",
    "- Note that these values are only approximate, so the Bellman equation doesn't hold exactly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62da349",
   "metadata": {},
   "source": [
    "<img src='https://github.com/tombewley/one-hour-rl/blob/main/images/student-mdp-Q-values.svg?raw=true' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1165f",
   "metadata": {},
   "source": [
    "To take one example:\n",
    "$$\n",
    "Q(\\text{Class 2},\\text{Study})=-2+0.95\\times(0.6\\times Q(\\text{Class 3},\\text{Study})+0.4\\times Q(\\text{Class 3},\\text{Go to the pub}))\n",
    "$$\n",
    "$$\n",
    "=-2+0.95\\times(0.6\\times 9.99+0.4\\times 1.81)=4.38\\approx 4.36\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca0a3a",
   "metadata": {},
   "source": [
    "How did we arrive at these Q value estimates? Here's where the real magic happens.\n",
    "\n",
    "The *Bellman backup* algorithm makes use of this recursive relationship to update the Q value for a state-action pair based on the *current estimate of the value for the next state*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aed6dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95  # Discount factor\n",
    "ALPHA = 0.001 # Learning rate\n",
    "\n",
    "def bellman_backup(agent, state, action, reward, next_state, done):\n",
    "\n",
    "    Q_next = 0. if done else agent.Q[next_state][agent.act(next_state)]\n",
    "\n",
    "    agent.Q[state][action] += ALPHA * ( reward + GAMMA * Q_next - agent.Q[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a540655",
   "metadata": {},
   "source": [
    "By sampling episodes in our MDP using the current policy we can collect rewards and update our Q-function accordingly. The algorithm we use to evaluate policies is called policy evaluation, and it uses the Bellman back-up which has two hyperparameters $\\gamma$ and $\\alpha$. $\\gamma$ is the discount factor that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e0edf",
   "metadata": {},
   "source": [
    "Import the MDP and define the policy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d707baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import StudentMDP\n",
    "from agent import Agent\n",
    "mdp = StudentMDP(verbose=True)\n",
    "agent = Agent(mdp) \n",
    "agent.policy = {\n",
    "    \"Class 1\":  {\"Study\": 0.5, \"Go on Facebook\": 0.5},\n",
    "    \"Class 2\":  {\"Study\": 0.8, \"Fall asleep\": 0.2},\n",
    "    \"Class 3\":  {\"Study\": 0.6, \"Go to the pub\": 0.4},\n",
    "    \"Facebook\": {\"Keep scrolling\": 0.9, \"Close Facebook\": 0.1},\n",
    "    \"Pub\":      {\"Have a pint\": 1.},\n",
    "    \"Pass\":     {\"Fall asleep\": 1.},\n",
    "    \"Asleep\":   {\"Stay asleep\": 1.}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7bb0ac",
   "metadata": {},
   "source": [
    "Initially, we set all Q values to zero (this is actually arbitrary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65261081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class 1': {'Study': 0.0, 'Go on Facebook': 0.0},\n",
       " 'Class 2': {'Study': 0.0, 'Fall asleep': 0.0},\n",
       " 'Class 3': {'Study': 0.0, 'Go to the pub': 0.0},\n",
       " 'Facebook': {'Keep scrolling': 0.0, 'Close Facebook': 0.0},\n",
       " 'Pub': {'Have a pint': 0.0},\n",
       " 'Pass': {'Fall asleep': 0.0},\n",
       " 'Asleep': {}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981baec",
   "metadata": {},
   "source": [
    "Run a single episode to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c3b687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== EPISODE   2 ===========================\n",
      "| Time  | State    | Action         | Reward | Next state | Done  |\n",
      "|-------|----------|----------------|--------|------------|-------|\n",
      "| 0     | Class 1  | Study          | -2.0   | Class 2    | False |\n",
      "-0.002\n",
      "-2.0\n",
      "0.0\n",
      "-0.003998\n",
      "| 1     | Class 2  | Study          | -2.0   | Class 3    | False |\n",
      "0.0\n",
      "-2.0\n",
      "0.0\n",
      "-0.002\n",
      "| 2     | Class 3  | Study          | 10.0   | Pass       | False |\n",
      "0.0\n",
      "10.0\n",
      "0.0\n",
      "0.01\n",
      "| 3     | Pass     | Fall asleep    |  0.0   | Asleep     | True  |\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "state = mdp.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, _ = mdp.step(action)\n",
    "\n",
    "    print(agent.Q[state][action])\n",
    "    print(reward)\n",
    "    print(0. if done else agent.Q[next_state][agent.act(next_state)])\n",
    "\n",
    "    bellman_backup(agent, state, action, reward, next_state, done)\n",
    "\n",
    "    print(agent.Q[state][action])\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a7240",
   "metadata": {},
   "source": [
    "Repeating a bunch of times, we gradually converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.verbose = False\n",
    "for _ in range(20000):\n",
    "    state = mdp.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = mdp.step(action)\n",
    "        bellman_backup(agent, state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "    print(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35849b8b",
   "metadata": {},
   "source": [
    "Note that although the policy evaluation process is guaranteed to converge eventually (for simple MDPs!), we are likely to see some discrepencies between runs of finite length because of the role of randomness in the data collection process. Here are the results of five independent repeats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Class 1': {'Study': 1.2650695038546025, 'Go on Facebook': -11.30468184426212}, 'Class 2': {'Study': 4.407552596737938, 'Fall asleep': 0.0}, 'Class 3': {'Study': 9.99999695776742, 'Go to the pub': 1.8487809354712246}, 'Facebook': {'Keep scrolling': -11.258053618560483, 'Close Facebook': -6.489974573408375}, 'Pub': {'Have a pint': 0.9454014270087486}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}\n",
    "{'Class 1': {'Study': 1.3338704627380917, 'Go on Facebook': -11.222578014516461}, 'Class 2': {'Study': 4.404498313710967, 'Fall asleep': 0.0}, 'Class 3': {'Study': 9.999996607231745, 'Go to the pub': 1.9330819535637127}, 'Facebook': {'Keep scrolling': -11.237574593720579, 'Close Facebook': -6.649035509952115}, 'Pub': {'Have a pint': 1.0198591832482675}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}\n",
    "{'Class 1': {'Study': 1.255108027766012, 'Go on Facebook': -11.190843458457234}, 'Class 2': {'Study': 4.3028079916966, 'Fall asleep': 0.0}, 'Class 3': {'Study': 9.999996368375, 'Go to the pub': 1.692402249138645}, 'Facebook': {'Keep scrolling': -11.009224020468848, 'Close Facebook': -6.456279660637165}, 'Pub': {'Have a pint': 0.7467114530860179}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}\n",
    "{'Class 1': {'Study': 1.2734946938741027, 'Go on Facebook': -11.328006914127434}, 'Class 2': {'Study': 4.256107269897298, 'Fall asleep': 0.0}, 'Class 3': {'Study': 9.99999635381211, 'Go to the pub': 1.74113336614775}, 'Facebook': {'Keep scrolling': -11.34039736455563, 'Close Facebook': -6.777709970724558}, 'Pub': {'Have a pint': 0.7694312629253455}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}\n",
    "{'Class 1': {'Study': 1.2650695038546025, 'Go on Facebook': -11.30468184426212}, 'Class 2': {'Study': 4.407552596737938, 'Fall asleep': 0.0}, 'Class 3': {'Study': 9.99999695776742, 'Go to the pub': 1.8487809354712246}, 'Facebook': {'Keep scrolling': -11.258053618560483, 'Close Facebook': -6.489974573408375}, 'Pub': {'Have a pint': 0.9454014270087486}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b0d1e",
   "metadata": {},
   "source": [
    "Try with $\\gamma=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08053451",
   "metadata": {},
   "source": [
    "# 3 | Policy Improvement: The Q-learning Algorithm\n",
    "\n",
    "Having evaluated our policy $\\pi$, how can we go about obtaining a better one? This question is the heart of *policy improvement*, perhaps the fundamental concept of RL. Recall, when we performed policy evaluation we obtained the value of taking every action in every state. Thus, we can perform policy improvement readily by picking our current best estimate of the optimal action from each state -- so-called *greedy* action selection. Once we've obtained a new policy, we can evaluate it as before. Continually iterating between policy evaluation and policy improvement in this way, we are guarenteed to reach the optimal policy $\\pi^*$ according to the policy improvement theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d671f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import StudentMDP\n",
    "mdp = StudentMDP(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import QLearningAgent\n",
    "agent = QLearningAgent(mdp, epsilon=1.0, alpha=0.2, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d23e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPS = 50\n",
    "mdp.ep = 0\n",
    "while mdp.ep < NUM_EPS:\n",
    "    state = mdp.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = mdp.step(action)\n",
    "        agent.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "    print(\"Value function:\")\n",
    "    print(agent.Q)\n",
    "    print(\"Policy:\")\n",
    "    print(agent.policy)\n",
    "    print(\"Epsilon:\", agent.epsilon)\n",
    "    \n",
    "    agent.epsilon = max(agent.epsilon - 1 / (NUM_EPS-1), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b688ab",
   "metadata": {},
   "source": [
    "Try with $\\gamma=0$ and different $\\epsilon$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a4d06",
   "metadata": {},
   "source": [
    "# 4 | Deep RL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72902834",
   "metadata": {},
   "source": [
    "So far, we've tabularised the state-action space. Whilst useful for explaining the fundamental concepts that underpin RL, the real world state-action spaces are generally continuous and thus impossible to tabularise. To combat this, function approximators are used instead. In the past these included x, but more recently, deep neural networks have been used giving rise to the field of Deep Reinforcement Learning.\n",
    "\n",
    "The seminal Deep RL algorithm is Deep Q Learning which uses neural networks to represent the $Q$ function. The network takes the current obervation $o_t$ as input and predicts the value of each action. The agent's policy is $\\epsilon$-greedy as before i.e. it takes the value-maximising action with probability $1 - \\epsilon$. Deep Q learning \n",
    "\n",
    "Below, we run 500 episodes of the canonical Cartpole task using Deep Q learning. The agent's goal is to balance the pole in the upright position for as long as possible starting from an initially random position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from dqn_agent import Agent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0854f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent(gamma=0.99, epsilon=0.9, lr=0.0001, n_actions=env.action_space.n, input_dims=[env.observation_space.shape[0]],\n",
    "              mem_size=50000, batch_size=128,  eps_dec=1e-3, eps_min=0.05, replace=1000,\n",
    "              env_name='cartpole', chkpt_dir='tmp/dqn')\n",
    "\n",
    "best_score = -np.inf\n",
    "episodes = 500\n",
    "scores, avg_score, eps_history = [], [], []\n",
    "\n",
    "for i in range(episodes):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    env.render()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "        env.render()\n",
    "    \n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "    \n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    \n",
    "    print('episode', i, 'score %.2f' % score, 'average score %.2f' % avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "figure, ax1 = plt.subplots(1,1, figsize=(12,7.5))\n",
    "ax1.plot(np.arange(episodes), avg_score, label='rolling avg reward', color='blue')\n",
    "ax1.set_xlabel('episodes')\n",
    "ax1.set_ylabel('score', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.arange(episodes), eps_history, label='epsilon', color='red')\n",
    "ax2.set_ylabel('epsilon', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f766683",
   "metadata": {},
   "source": [
    "# 5 | What Did We Miss Out?\n",
    "\n",
    "- Dynamic programming (when transition probabilities are known)\n",
    "- Monte Carlo\n",
    "- Exploration strategies\n",
    "- Continuous actions\n",
    "- Policy gradient, actor-critic\n",
    "- Model-based\n",
    "- Partial observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c86e8f",
   "metadata": {},
   "source": [
    "What next? RL interest group?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
