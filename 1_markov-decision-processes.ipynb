{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 | Markov Decision Processes: A Model of Sequential Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. MDP (semi-)Formalism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning (RL), an *agent* takes *actions* in an *environment* to change its state, with the goal of maximising the expected sum of future *rewards*. We formalise this interaction as an agent-environment loop, mathematically described as a Markov Decision Process (MDP). \n",
    "\n",
    "<img src='https://github.com/enjeeneer/sutton_and_barto/blob/main/images/chapter3_1.png?raw=true' width='700'>\n",
    "\n",
    "MDPs break the I.I.D. data assumption of supervised and unsupervised learning; the agent *causally influences* the data it sees through its choice of actions. However, one assumption we do make is the *Markov property*, which says that the state representation captures *all relevent information* from the past. Formally, state transitions depend only on the most recent state and action,\n",
    "$$\n",
    "\\mathbb{P}[S_{t+1} | S_t,A_t] = \\mathbb{P}[S_{t+1} | S_1,A_1 \\ldots\\, S_t,A_t],\n",
    "$$\n",
    "and rewards depend only on the most recent transition,\n",
    "$$\n",
    "\\mathbb{P}[R_{t+1} | S_t,A_t,S_{t+1}] = \\mathbb{P}[R_{t+1} | S_1,A_1 \\ldots\\, S_t,A_t,S_{t+1}].\n",
    "$$\n",
    "- Note: different sources use different notation here, but this is the most general.\n",
    "\n",
    "In some MDPs, a subset of states are designated as *terminal* (or *absorbing*). The agent-environment interaction loop ceases once a terminal state is reached.\n",
    "\n",
    "The goal of an RL agent is to pick actions that maximise the discounted cumulative sum of future rewards, also known as the *return* $G_t$:\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots + \\gamma^{T-t-1}R_{T},\n",
    "$$\n",
    "where $\\gamma\\in[0,1]$ is a discount factor and $T$ is the time of termination (may be $\\infty$).\n",
    "\n",
    "To do so, it needs the ability to forecast the reward-getting effect of taking each action $A$ in each state $S$, potentially many timesteps into the future. This *temporal credit assignment* problem is one of the key factors that makes RL so challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MDP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple MDP (courtesy of David Silver @ DeepMind/UCL), which we'll be using throughout this course.\n",
    "- White circle: non-terminal state\n",
    "- White square: terminal state\n",
    "- Black circle: action\n",
    "- <span style=\"color:green\">Green:</span> reward (depends only on $S_{t+1}$ here)\n",
    "- <span style=\"color:blue\">Blue:</span> state transition probability\n",
    "- <span style=\"color:red\">Red:</span> action probability for an exemplar policy\n",
    "- Note: edges with probability $1$ are unlabelled\n",
    "\n",
    "<img src='https://github.com/tombewley/one-hour-rl/blob/main/images/student-mdp.svg?raw=true' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open AI Gym](https://gym.openai.com/) provides a unified framework for testing and comparing RL algorithms, and offers a suite of MDPs that researchers can use to benchmark their work. It's important to be familiar with the conventions of Gym, because almost all modern RL code is built to work with it. Gym environment classes have two key methods:\n",
    "\n",
    "- `mdp.reset()`: resets the MDP to an initial state $S_0$ according to an initialisation distribution.\n",
    "- `mdp.step(action)` : takes an action $A_t$, combines with the current environment state $S_t$, produces the next state $S_{t+1}$ and delivers the agent a scalar reward $R_{t+1}$.\n",
    "\n",
    "A Gym-compatible class for the student MDP shown above can be found in `mdp.py` in this repository. Let's import it now and explore what it can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import StudentMDP\n",
    "mdp = StudentMDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we'll have a look at the initialisation probabilities and the behaviour of `mdp.reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Class 1': 1.0, 'Class 2': 0.0, 'Class 3': 0.0, 'Facebook': 0.0, 'Pub': 0.0, 'Pass': 0.0, 'Asleep': 0.0}\n",
      "Class 1\n"
     ]
    }
   ],
   "source": [
    "print(mdp.initial_probs())\n",
    "mdp.reset()\n",
    "print(mdp.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check which actions are available in this initial state, and the action-dependent transition probabilities.\n",
    "- Reminder: the Markov property dictates that transition probabilities depend *only* on the current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Go on Facebook', 'Study'}\n",
      "{'Class 2': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(mdp.action_space(mdp.state))\n",
    "print(mdp.transition_probs(mdp.state, \"Study\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `mdp.step(action)` samples and returns the next state $S_{t+1}$, alongside a scalar reward $R_{t+1}$.\n",
    "\n",
    "Let's try calling this method repeatedly. What's happening here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Study'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-da5a7936f65e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Study\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/Projects/one-hour-rl/mdp.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/Projects/one-hour-rl/mdp.py\u001b[0m in \u001b[0;36mreward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"actions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransition_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Study'"
     ]
    }
   ],
   "source": [
    "state, reward, _, _ = mdp.step(\"Study\") \n",
    "print(state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transitions out of the `Pub` state are *non-deterministic*; they go to one of the three classes with specified probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Have a pint'}\n",
      "{'Class 1': 0.2, 'Class 2': 0.4, 'Class 3': 0.4}\n"
     ]
    }
   ],
   "source": [
    "mdp.state = \"Pub\"\n",
    "print(mdp.action_space(mdp.state))\n",
    "print(mdp.transition_probs(mdp.state, \"Have a pint\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this state, the behaviour of `mdp.step(action)` is stochastic, even for a constant action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 -2.0\n"
     ]
    }
   ],
   "source": [
    "mdp.state = \"Pub\" # Note that we're resetting the state to Pub each time\n",
    "state, reward, _, _ = mdp.step(\"Have a pint\")\n",
    "print(state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MDP has just one terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Asleep'}\n"
     ]
    }
   ],
   "source": [
    "print(mdp.terminal_states())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mdp.step(action)` also returns a binary `done` flag, which is set to `True` if $S_{t+1}$ is a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asleep 0.0 True\n",
      "Asleep 0.0 True\n"
     ]
    }
   ],
   "source": [
    "mdp.state = \"Class 2\" \n",
    "state, reward, done, _ = mdp.step(\"Fall asleep\")\n",
    "print(state, reward, done)\n",
    "\n",
    "mdp.state = \"Pass\" \n",
    "state, reward, done, _ = mdp.step(\"Fall asleep\")\n",
    "print(state, reward, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring an agent into the mix, and give it the exemplar policy shown in the diagram above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "agent = Agent(mdp) \n",
    "agent.policy = {\n",
    "    \"Class 1\":  {\"Study\": 0.5, \"Go on Facebook\": 0.5},\n",
    "    \"Class 2\":  {\"Study\": 0.8, \"Fall asleep\": 0.2},\n",
    "    \"Class 3\":  {\"Study\": 0.6, \"Go to the pub\": 0.4},\n",
    "    \"Facebook\": {\"Keep scrolling\": 0.9, \"Close Facebook\": 0.1},\n",
    "    \"Pub\":      {\"Have a pint\": 1.},\n",
    "    \"Pass\":     {\"Fall asleep\": 1.},\n",
    "    \"Asleep\":   {\"Stay asleep\": 1.}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the policy in a similar way to the MDP's properties, and observe its stochastic behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Study': 0.5, 'Go on Facebook': 0.5}\n",
      "['Go on Facebook', 'Go on Facebook', 'Study', 'Study', 'Go on Facebook', 'Study', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Study', 'Study', 'Go on Facebook', 'Go on Facebook', 'Go on Facebook', 'Go on Facebook', 'Go on Facebook', 'Study', 'Study']\n"
     ]
    }
   ],
   "source": [
    "print(agent.policy[\"Class 1\"])\n",
    "print([agent.act(\"Class 1\") for _ in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== EPISODE   1 ===========================\n",
      "| Time  | State    | Action         | Reward | Next state | Done  |\n",
      "|-------|----------|----------------|--------|------------|-------|\n",
      "| 0     | Class 1  | Go on Facebook | -1.0   | Facebook   | False |\n",
      "| 1     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 2     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 3     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 4     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 5     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 6     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 7     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 8     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 9     | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 10    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 11    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 12    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 13    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 14    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 15    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 16    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 17    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 18    | Facebook | Keep scrolling | -1.0   | Facebook   | False |\n",
      "| 19    | Facebook | Close Facebook | -2.0   | Class 1    | False |\n",
      "| 20    | Class 1  | Study          | -2.0   | Class 2    | False |\n",
      "| 21    | Class 2  | Study          | -2.0   | Class 3    | False |\n",
      "| 22    | Class 3  | Study          | 10.0   | Pass       | False |\n",
      "| 23    | Pass     | Fall asleep    |  0.0   | Asleep     | True  |\n"
     ]
    }
   ],
   "source": [
    "mdp.verbose = True\n",
    "state = mdp.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    state, reward, done, info = mdp.step(agent.act(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How \"good\" is this policy? To answer this, we need to calculate its expected return."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
