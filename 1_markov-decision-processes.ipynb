{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 | Markov Decision Processes: A Model of Sequential Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/enjeeneer/sutton_and_barto/blob/main/images/chapter3_1.png?raw=true' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. MDP (semi-)Formalism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In RL, *agents* take *actions* in an *environment* to maximise the sum of its cumulative *rewards*.\n",
    "- Unlike supervised and unsupervised learning, we lose the I.I.D date assumption; the RL agent *affects* its dataset by taking actions that change its observations.\n",
    "- We formalise this interaction as the agent-environment loop, mathematically described as a Markov Decision Process (MDP).\n",
    "- In MDPs, we assume the state representation is *Markov* i.e. it captures all relevent information from the history:\n",
    "\n",
    "$\n",
    "\\mathbb{P}[S_{t+1} | S_t] = \\mathbb{P}[S_{t+1} | S_1, \\ldots\\, S_t]\n",
    "$\n",
    "\n",
    "- The goal of an RL agent is to pick actions that maximise the cumulative sum of future rewards, also known as the *discounted return* $G_t$:\n",
    "\n",
    "$\n",
    "G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\ldots + R_{T}\n",
    "$\n",
    "\n",
    "where $T$ is a terminal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MDP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The student MDP](images/student-mdp.png?raw=true \"The student MDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import StudentMDP\n",
    "mdp = StudentMDP(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time  | State    | Action         | Reward | Next state | Done  |\n",
      "|-------|----------|----------------|--------|------------|-------|\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StudentMDP' object has no attribute 't'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_107192/464217876.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Class 2\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Study\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\teaching\\one-hour-rl\\mdp.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Asleep\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             print((f\"| {str(self.t).ljust(5)} | {self.state.ljust(8)} | {action.ljust(14)} |\"\n\u001b[0m\u001b[0;32m     85\u001b[0m                    f\"{str(reward).rjust(5).ljust(7)} | {next_state.ljust(10)} | {str(done).ljust(5)} |\"))\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StudentMDP' object has no attribute 't'"
     ]
    }
   ],
   "source": [
    "mdp.state = \"Class 2\"\n",
    "mdp.print_header()\n",
    "mdp.step(\"Study\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default policy shown in images/student-mdp.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "agent = Agent(mdp.action_space) \n",
    "agent.pi = {\n",
    "    \"Class 1\":  {\"Study\": 0.5, \"Go on Facebook\": 0.5},\n",
    "    \"Class 2\":  {\"Study\": 0.8, \"Fall asleep\": 0.2},\n",
    "    \"Class 3\":  {\"Study\": 0.6, \"Go to the pub\": 0.4},\n",
    "    \"Facebook\": {\"Keep scrolling\": 0.9, \"Close Facebook\": 0.1},\n",
    "    \"Pub\":      {\"Have a pint\": 1.},\n",
    "    \"Pass\":     {\"Fall asleep\": 1.},\n",
    "    \"Asleep\":   {\"Stay asleep\": 1.}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Study': 0.5, 'Go on Facebook': 0.5}\n",
      "['Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Study', 'Study', 'Study', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Study', 'Study', 'Study']\n"
     ]
    }
   ],
   "source": [
    "print(agent.pi[\"Class 1\"])\n",
    "print([agent.act(\"Class 1\") for _ in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== EPISODE   5 ===========================\n",
      "| Time  | State    | Action         | Reward | Next state | Done  |\n",
      "|-------|----------|----------------|--------|------------|-------|\n",
      "| 0     | Class 1  | Study          | -2.0   | Class 2    | False |\n",
      "| 1     | Class 2  | Study          | -2.0   | Class 3    | False |\n",
      "| 2     | Class 3  | Study          | 10.0   | Pass       | False |\n",
      "| 3     | Pass     | Fall asleep    |  0.0   | Asleep     | True  |\n"
     ]
    }
   ],
   "source": [
    "state = mdp.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    state, _, done, _ = mdp.step(agent.act(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open AI gym](https://gym.openai.com/) is the premier toolkit for testing and comparing RL algorithms. They offer a suite of MDPs that RL researchers use to benchmark their work. It's important to be familiar with their conventions because all modern RL problems are built a-top their framework. *Gym* environment classes have two key methods:\n",
    "\n",
    "- reset(): resets the MDP to an initial state that may/may not be random\n",
    "- step() : takes an action $a_t$, combines with the current environment state $s_t$ and produces the next state $s_{t+1}$ and delivers the agent a reward $r_t$.\n",
    "\n",
    "The core *Gym* environment class can be found "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
