{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 | Markov Decision Processes: A Model of Sequential Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/enjeeneer/sutton_and_barto/blob/main/images/chapter3_1.png?raw=true' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. MDP (semi-)Formalism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In RL, an *agent* takes *actions* in an *environment* to maximise the sum of its cumulative *rewards*.\n",
    "- Unlike supervised and unsupervised learning, we lose the I.I.D. data assumption; the RL agent *affects* its dataset by taking actions that change its observations.\n",
    "- We formalise this interaction as the agent-environment loop, mathematically described as a Markov Decision Process (MDP).\n",
    "- In MDPs, we assume the state representation is *Markov* i.e. it captures all relevent information from the history:\n",
    "$$\n",
    "\\mathbb{P}[S_{t+1} | S_t] = \\mathbb{P}[S_{t+1} | S_1, \\ldots\\, S_t]\n",
    "$$\n",
    "- The goal of an RL agent is to pick actions that maximise the cumulative sum of future rewards, also known as the *discounted return* $G_t$:\n",
    "$$\n",
    "G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\ldots + R_{T}\n",
    "$$\n",
    "where $T$ is a terminal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MDP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update this diagram. It's a bit misleading because it represents the stochastic *policy* and stochastic *dynamics* in the same way.\n",
    "\n",
    "![The student MDP](https://github.com/tombewley/one-hour-rl/blob/main/images/student-mdp.png?raw=true \"The student MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open AI Gym](https://gym.openai.com/) is the premier toolkit for testing and comparing RL algorithms. They offer a suite of MDPs that RL researchers use to benchmark their work. It's important to be familiar with their conventions because all modern RL problems are built a-top their framework. *Gym* environment classes have two key methods:\n",
    "\n",
    "- reset(): resets the MDP to an initial state that may/may not be random\n",
    "- step() : takes an action $a_t$, combines with the current environment state $s_t$ and produces the next state $s_{t+1}$ and delivers the agent a reward $r_t$.\n",
    "\n",
    "The core *Gym* environment class can be found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import StudentMDP\n",
    "mdp = StudentMDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce initialisation probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Class 1': 1.0, 'Class 2': 0.0, 'Class 3': 0.0, 'Facebook': 0.0, 'Pub': 0.0, 'Pass': 0.0, 'Asleep': 0.0}\n",
      "Class 1\n"
     ]
    }
   ],
   "source": [
    "print(mdp.initial_probs())\n",
    "mdp.reset()\n",
    "print(mdp.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce action space and transition probabilities. Reminder of Markov property: only dependent on current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Go on Facebook', 'Study'}\n",
      "{'Class 2': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(mdp.action_space(mdp.state))\n",
    "print(mdp.transition_probs(mdp.state, \"Study\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mdp.step(action) returns the state and reward.\n",
    "\n",
    "Run repeatedly... what's happening here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 2 -2.0\n"
     ]
    }
   ],
   "source": [
    "state, reward, _, _ = mdp.step(\"Study\") \n",
    "print(state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-deterministic transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Have a pint'}\n",
      "{'Class 1': 0.2, 'Class 2': 0.4, 'Class 3': 0.4}\n"
     ]
    }
   ],
   "source": [
    "mdp.state = \"Pub\"\n",
    "print(mdp.action_space(mdp.state))\n",
    "print(mdp.transition_probs(mdp.state, \"Have a pint\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run repeatedly... what's happening here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 3\n"
     ]
    }
   ],
   "source": [
    "mdp.state = \"Pub\" # Note that we're resetting the state to \"Pub\" each time\n",
    "state, reward, _, _ = mdp.step(\"Have a pint\")\n",
    "print(state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce terminal probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Class 1': 0.0, 'Class 2': 0.0, 'Class 3': 0.0, 'Facebook': 0.0, 'Pub': 0.0, 'Pass': 0.0, 'Asleep': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(mdp.done_probs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mdp.step(action) also returns a \"done\" flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asleep 0.0 True\n",
      "Asleep 0.0 True\n"
     ]
    }
   ],
   "source": [
    "mdp.state = \"Class 2\" \n",
    "state, reward, done, _ = mdp.step(\"Fall asleep\")\n",
    "print(state, reward, done)\n",
    "\n",
    "mdp.state = \"Pass\" \n",
    "state, reward, done, _ = mdp.step(\"Fall asleep\")\n",
    "print(state, reward, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default policy shown in images/student-mdp.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "agent = Agent(mdp) \n",
    "agent.policy = {\n",
    "    \"Class 1\":  {\"Study\": 0.5, \"Go on Facebook\": 0.5},\n",
    "    \"Class 2\":  {\"Study\": 0.8, \"Fall asleep\": 0.2},\n",
    "    \"Class 3\":  {\"Study\": 0.6, \"Go to the pub\": 0.4},\n",
    "    \"Facebook\": {\"Keep scrolling\": 0.9, \"Close Facebook\": 0.1},\n",
    "    \"Pub\":      {\"Have a pint\": 1.},\n",
    "    \"Pass\":     {\"Fall asleep\": 1.},\n",
    "    \"Asleep\":   {\"Stay asleep\": 1.}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Study': 0.5, 'Go on Facebook': 0.5}\n",
      "['Study', 'Go on Facebook', 'Go on Facebook', 'Go on Facebook', 'Go on Facebook', 'Go on Facebook', 'Study', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Study', 'Study']\n"
     ]
    }
   ],
   "source": [
    "print(agent.policy[\"Class 1\"])\n",
    "print([agent.act(\"Class 1\") for _ in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== EPISODE   7 ===========================\n",
      "| Time  | State    | Action         | Reward | Next state | Done  |\n",
      "|-------|----------|----------------|--------|------------|-------|\n",
      "| 0     | Class 1  | Study          | -2.0   | Class 2    | False |\n",
      "| 1     | Class 2  | Study          | -2.0   | Class 3    | False |\n",
      "| 2     | Class 3  | Study          | 10.0   | Pass       | False |\n",
      "| 3     | Pass     | Fall asleep    |  0.0   | Asleep     | True  |\n"
     ]
    }
   ],
   "source": [
    "mdp.verbose = True\n",
    "state = mdp.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    state, reward, done, info = mdp.step(agent.act(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How \"good\" is this policy?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
